{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKRiM-IvNEbn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gDDvsQ5QDK_",
        "outputId": "3137e5df-e396-405b-9968-e0e7aac47b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptEmk08YQJyc",
        "outputId": "b6e5f338-f52b-457d-d6f3-1e8cf48f6dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWLT7LRGQgy1",
        "outputId": "22b26a01-1eaa-406c-a47b-23897395e867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRrBGTONRZsY",
        "outputId": "46bad04f-e13d-4b59-c745-6c79df36a7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P25SvKzSNOnn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOSlau50NJGZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"roy_lyrics.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLsacdAtR-6F",
        "outputId": "b5fb2150-b3d4-47a9-e02e-4f35a34abe42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting text2emotion\n",
            "  Downloading text2emotion-0.0.5-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji>=0.6.0\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from text2emotion) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->text2emotion) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->text2emotion) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->text2emotion) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->text2emotion) (4.65.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=ac14020f2f98ecc7ae789273ff9bce0cdc5e5cd59f79c95301f049db80d0ebed\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, text2emotion\n",
            "Successfully installed emoji-2.2.0 text2emotion-0.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install text2emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvU6YmFhU4nU",
        "outputId": "0d263bfb-531c-489a-ad21-d37fe8d9c1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji==1.6.3\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=f6c7b32208979229250febb1f93d49a8d3021cfccc81b7863102522cb7434b70\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/42/4a/2e5a7b28b80e3a4f143cb5bbfcfbfe54958ccb533bb8ce2a99\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "  Attempting uninstall: emoji\n",
            "    Found existing installation: emoji 2.2.0\n",
            "    Uninstalling emoji-2.2.0:\n",
            "      Successfully uninstalled emoji-2.2.0\n",
            "Successfully installed emoji-1.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji==1.6.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0mVAaGKSzAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8548e0-85d4-414b-df4c-8f2e2b3dfab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Import the modules\n",
        "import text2emotion as te"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5COSvVnpWgN-"
      },
      "outputs": [],
      "source": [
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WJRnbeCWinU"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXF-ivGWhsw"
      },
      "outputs": [],
      "source": [
        "sys.modules[\"sklearn.svm.classes\"] = sklearn.svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGTJL1VfVfYU",
        "outputId": "e9db5a81-110e-40ff-81b8-b1d3447b2834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting alt-profanity-check\n",
            "  Downloading alt-profanity-check-1.2.2.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from alt-profanity-check) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from alt-profanity-check) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->alt-profanity-check) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->alt-profanity-check) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->alt-profanity-check) (1.10.1)\n",
            "Building wheels for collected packages: alt-profanity-check\n",
            "  Building wheel for alt-profanity-check (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alt-profanity-check: filename=alt_profanity_check-1.2.2-py3-none-any.whl size=1866179 sha256=d6f43a80096aee82b21db25d12c0d8672263b46be067e40e54ed37395dcd2425\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/98/77/c2903d8f2862ecf6ac3f51007e82f12d456f1ac7f6a147e7ab\n",
            "Successfully built alt-profanity-check\n",
            "Installing collected packages: alt-profanity-check\n",
            "Successfully installed alt-profanity-check-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install alt-profanity-check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWr08xZqX0oa"
      },
      "outputs": [],
      "source": [
        "from profanity_check import predict, predict_prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words_list = []\n",
        "vocab_diversity_list = []\n",
        "pos_sen_list = []\n",
        "neg_sen_list = []\n",
        "happy_list = []\n",
        "fear_list = []\n",
        "anger_list = []\n",
        "sad_list = []\n",
        "surprise_list = []\n",
        "profanity_list = []"
      ],
      "metadata": {
        "id": "P_jaz-FEohv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkSMMh45YVWK"
      },
      "outputs": [],
      "source": [
        "def get_features(song_lyrics):\n",
        "  # Tokenize the lyrics into words\n",
        "    words = word_tokenize(lyrics)\n",
        "\n",
        "    # Count the total number of words\n",
        "    word_count = len(words)\n",
        "    num_words_list.append(word_count)\n",
        "\n",
        "    # Measure vocabulary diversity\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w.lower()) for w in words if w.isalpha()]\n",
        "    vocab_diversity = len(set(lemmatized_words)) / len(lemmatized_words)\n",
        "\n",
        "    vocab_diversity_list.append(vocab_diversity)\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment = sia.polarity_scores(lyrics)\n",
        "    sentiment_pos = sentiment['pos']\n",
        "    sentiment_neg = sentiment['neg']\n",
        "    sentiment_neu = sentiment['neu']\n",
        "\n",
        "    pos_sen_list.append(sentiment_pos)\n",
        "    neg_sen_list.append(sentiment_neg)\n",
        "\n",
        "    emotions = te.get_emotion(lyrics)\n",
        "    happy_list.append(emotions[\"Happy\"])\n",
        "    fear_list.append(emotions[\"Fear\"])\n",
        "    anger_list.append(emotions[\"Angry\"])\n",
        "    sad_list.append(emotions[\"Sad\"])\n",
        "    surprise_list.append(emotions[\"Surprise\"])\n",
        "\n",
        "    prof_score = predict_prob([lyrics])\n",
        "    profanity_list.append(prof_score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NXQhYUGaGmQ"
      },
      "outputs": [],
      "source": [
        "lyric_list = list(df[\"lyrics\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for lyrics in lyric_list:\n",
        "  if not pd.isna(lyrics):\n",
        "    get_features(lyrics)"
      ],
      "metadata": {
        "id": "5xPa-IXdpYh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['lyrics'].notnull()]\n",
        "df[\"num_words\"] = num_words_list\n",
        "df[\"vocab_diveristy\"] = vocab_diversity_list\n",
        "df[\"pos_sen\"] = pos_sen_list\n",
        "df[\"neg_sen\"] = neg_sen_list"
      ],
      "metadata": {
        "id": "FlNTwCOAp11j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "_GL2g6lLr3Tn",
        "outputId": "4c04c7b2-55e3-439a-ac85-a5ef4b8f46b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     index  year      award category                        song  \\\n",
              "106    107  1980  Record of the year        What A Fool Believes   \n",
              "107    108  1980  Record of the year  You Don't Bring Me Flowers   \n",
              "108    109  1980  Record of the year        The Gambler (Single)   \n",
              "109    110  1980  Record of the year     I Will Survive (Single)   \n",
              "110    111  1980  Record of the year     After The Love Has Gone   \n",
              "..     ...   ...                 ...                         ...   \n",
              "341    342  2023  Record of the year                       Woman   \n",
              "342    343  2023  Record of the year      You And Me On The Rock   \n",
              "343    344  2023  Record of the year       Good Morning Gorgeous   \n",
              "344    345  2023  Record of the year                  Easy On Me   \n",
              "345    346  2023  Record of the year          Don't Shut Me Down   \n",
              "\n",
              "                                artist wasWinner  \\\n",
              "106                The Doobie Brothers       yes   \n",
              "107  Barbra Streisand And Neil Diamond        no   \n",
              "108                       Kenny Rogers        no   \n",
              "109                      Gloria Gaynor        no   \n",
              "110                 Earth; Wind & Fire        no   \n",
              "..                                 ...       ...   \n",
              "341                           Doja Cat        no   \n",
              "342    Brandi Carlile Featuring Lucius        no   \n",
              "343                      Mary J. Blige        no   \n",
              "344                              Adele        no   \n",
              "345                               ABBA        no   \n",
              "\n",
              "                                                lyrics  num_words  \\\n",
              "106  he came from somewhere back in her long ago th...        279   \n",
              "107  you dont bring me flowers you dont sing me lov...        224   \n",
              "108  on a warm summers evenin on a train bound for ...        421   \n",
              "109  at first i was afraid i was petrified kept thi...        650   \n",
              "110  for awhile to love was all we could do we were...        367   \n",
              "..                                                 ...        ...   \n",
              "341  you you love it how i move you you love it how...        446   \n",
              "342  they build wooden houses on frozen ponds in th...        408   \n",
              "343  ooh ooh ooh ohh ah ooh ooh    its so hard just...        338   \n",
              "344  there aint no gold in this river that ive been...        218   \n",
              "345  a while ago i heard the sound of childrens lau...        383   \n",
              "\n",
              "     vocab_diveristy  pos_sen  neg_sen  \n",
              "106         0.394265    0.211    0.073  \n",
              "107         0.392857    0.126    0.085  \n",
              "108         0.377672    0.032    0.044  \n",
              "109         0.240000    0.151    0.171  \n",
              "110         0.264305    0.230    0.122  \n",
              "..               ...      ...      ...  \n",
              "341         0.237668    0.267    0.000  \n",
              "342         0.357843    0.041    0.068  \n",
              "343         0.363905    0.330    0.121  \n",
              "344         0.444954    0.201    0.052  \n",
              "345         0.378590    0.160    0.104  \n",
              "\n",
              "[239 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3715f62e-7c43-4a73-9b13-59db2903b363\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>year</th>\n",
              "      <th>award category</th>\n",
              "      <th>song</th>\n",
              "      <th>artist</th>\n",
              "      <th>wasWinner</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>num_words</th>\n",
              "      <th>vocab_diveristy</th>\n",
              "      <th>pos_sen</th>\n",
              "      <th>neg_sen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>107</td>\n",
              "      <td>1980</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>What A Fool Believes</td>\n",
              "      <td>The Doobie Brothers</td>\n",
              "      <td>yes</td>\n",
              "      <td>he came from somewhere back in her long ago th...</td>\n",
              "      <td>279</td>\n",
              "      <td>0.394265</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>108</td>\n",
              "      <td>1980</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>You Don't Bring Me Flowers</td>\n",
              "      <td>Barbra Streisand And Neil Diamond</td>\n",
              "      <td>no</td>\n",
              "      <td>you dont bring me flowers you dont sing me lov...</td>\n",
              "      <td>224</td>\n",
              "      <td>0.392857</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>109</td>\n",
              "      <td>1980</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>The Gambler (Single)</td>\n",
              "      <td>Kenny Rogers</td>\n",
              "      <td>no</td>\n",
              "      <td>on a warm summers evenin on a train bound for ...</td>\n",
              "      <td>421</td>\n",
              "      <td>0.377672</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>110</td>\n",
              "      <td>1980</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>I Will Survive (Single)</td>\n",
              "      <td>Gloria Gaynor</td>\n",
              "      <td>no</td>\n",
              "      <td>at first i was afraid i was petrified kept thi...</td>\n",
              "      <td>650</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>111</td>\n",
              "      <td>1980</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>After The Love Has Gone</td>\n",
              "      <td>Earth; Wind &amp; Fire</td>\n",
              "      <td>no</td>\n",
              "      <td>for awhile to love was all we could do we were...</td>\n",
              "      <td>367</td>\n",
              "      <td>0.264305</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>342</td>\n",
              "      <td>2023</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>Woman</td>\n",
              "      <td>Doja Cat</td>\n",
              "      <td>no</td>\n",
              "      <td>you you love it how i move you you love it how...</td>\n",
              "      <td>446</td>\n",
              "      <td>0.237668</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>343</td>\n",
              "      <td>2023</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>You And Me On The Rock</td>\n",
              "      <td>Brandi Carlile Featuring Lucius</td>\n",
              "      <td>no</td>\n",
              "      <td>they build wooden houses on frozen ponds in th...</td>\n",
              "      <td>408</td>\n",
              "      <td>0.357843</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>344</td>\n",
              "      <td>2023</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>Good Morning Gorgeous</td>\n",
              "      <td>Mary J. Blige</td>\n",
              "      <td>no</td>\n",
              "      <td>ooh ooh ooh ohh ah ooh ooh    its so hard just...</td>\n",
              "      <td>338</td>\n",
              "      <td>0.363905</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>345</td>\n",
              "      <td>2023</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>Easy On Me</td>\n",
              "      <td>Adele</td>\n",
              "      <td>no</td>\n",
              "      <td>there aint no gold in this river that ive been...</td>\n",
              "      <td>218</td>\n",
              "      <td>0.444954</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>346</td>\n",
              "      <td>2023</td>\n",
              "      <td>Record of the year</td>\n",
              "      <td>Don't Shut Me Down</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>no</td>\n",
              "      <td>a while ago i heard the sound of childrens lau...</td>\n",
              "      <td>383</td>\n",
              "      <td>0.378590</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>239 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3715f62e-7c43-4a73-9b13-59db2903b363')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3715f62e-7c43-4a73-9b13-59db2903b363 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3715f62e-7c43-4a73-9b13-59db2903b363');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"happy\"] = happy_list\n",
        "df[\"fear\"] = fear_list\n",
        "df[\"anger\"] = anger_list\n",
        "df[\"sad\"] = sad_list\n",
        "df[\"surprise\"] = surprise_list\n",
        "df[\"profanity\"] = profanity_list"
      ],
      "metadata": {
        "id": "jiphL274p137"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"roy_lyrics_features.csv\")"
      ],
      "metadata": {
        "id": "aDzroBJMp16Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRYB4ud6p19G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9ONxnzvp1_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "PHfxqZh6cNdW",
        "outputId": "c3acbd6b-ccf0-4056-9bbc-c0a9af66e7fd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-da2cf969367f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained GloVe model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove-wiki-gigaword-50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split lyrics into individual words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
          ]
        }
      ],
      "source": [
        "# Load pre-trained GloVe model\n",
        "model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "\n",
        "# Split lyrics into individual words\n",
        "words = lyrics.split()\n",
        "\n",
        "# Get vector representation for each word\n",
        "word_vectors = []\n",
        "for word in words:\n",
        "    try:\n",
        "        vector = model[word]\n",
        "        word_vectors.append(vector)\n",
        "    except KeyError:\n",
        "        # Ignore words that are not in the vocabulary\n",
        "        pass\n",
        "\n",
        "# Calculate mean vector\n",
        "if len(word_vectors) > 0:\n",
        "    song_vector = np.mean(word_vectors, axis=0)\n",
        "else:\n",
        "    # Handle case where no words are in the vocabulary\n",
        "    song_vector = np.zeros((300,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "7CS6nWcgcR6u",
        "outputId": "0f089c01-17f4-4694-ec1f-f7029f6e47bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-da2cf969367f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained GloVe model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove-wiki-gigaword-50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split lyrics into individual words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
          ]
        }
      ],
      "source": [
        "# Load pre-trained GloVe model\n",
        "model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "\n",
        "# Split lyrics into individual words\n",
        "words = lyrics.split()\n",
        "\n",
        "# Get vector representation for each word\n",
        "word_vectors = []\n",
        "for word in words:\n",
        "    try:\n",
        "        vector = model[word]\n",
        "        word_vectors.append(vector)\n",
        "    except KeyError:\n",
        "        # Ignore words that are not in the vocabulary\n",
        "        pass\n",
        "\n",
        "# Calculate mean vector\n",
        "if len(word_vectors) > 0:\n",
        "    song_vector = np.mean(word_vectors, axis=0)\n",
        "else:\n",
        "    # Handle case where no words are in the vocabulary\n",
        "    song_vector = np.zeros((300,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "AQ-sANxxckHc",
        "outputId": "7e568814-a519-47c3-c48c-96dc1cc94b54"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9998ff684eda>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msong_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'song_vector' is not defined"
          ]
        }
      ],
      "source": [
        "song_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73D_AavgcwHT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "c3ba78af-f1a0-4fe7-f726-ada34d8af4f5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-f20700350cb2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained GloVe model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove-twitter-50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split lyrics into individual words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
          ]
        }
      ],
      "source": [
        "# Load pre-trained GloVe model\n",
        "model = api.load('glove-twitter-50')\n",
        "\n",
        "\n",
        "# Split lyrics into individual words\n",
        "words = lyrics.split()\n",
        "\n",
        "# Get vector representation for each word\n",
        "word_vectors = []\n",
        "for word in words:\n",
        "    try:\n",
        "        vector = model[word]\n",
        "        word_vectors.append(vector)\n",
        "    except KeyError:\n",
        "        # Ignore words that are not in the vocabulary\n",
        "        pass\n",
        "\n",
        "# Calculate mean vector\n",
        "if len(word_vectors) > 0:\n",
        "    song_vector = np.mean(word_vectors, axis=0)\n",
        "else:\n",
        "    # Handle case where no words are in the vocabulary\n",
        "    song_vector = np.zeros((300,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3uES36gdspg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}